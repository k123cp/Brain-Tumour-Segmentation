{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of A3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NpuIsoPtSpu"
      },
      "source": [
        "### Readme\n",
        "- The dataset has been uploaded, since registering and requesting access to the data source takes time, and there is no permanent link available. This means there are paths in the notebook that need to be changed according to your Drive structure, these have been marked with a comment.\n",
        "- The model is built and trained in the .ipynb notebook, to output a .hdf5 model file for the GUI to use. \n",
        "\n",
        "\n",
        "- To use the GUI, press 'Browse' and follow the prompts to load the trained model, choose brain modalities to predict, and perform segmentation.\n",
        "- The folder containing the brain modalities to use/predict when testing the GUI need to contain 4 nii.gz files (for t1, t2, flair, and t1ce) (from the same brain).\n",
        "- The resulting nii.gz file containing the predicted segmentation is saved in the same parent folder as the folder containing the brain modalities, named as: modalities folder's name + 'prediction.nii.gz'. For example, the folder containing the t1, t2, flair, and t1ce modalities is names 'brain1', then the prediction result is saved as 'brain1prediction.nii.gz' in brain1's parent folder.\n",
        "- There might be lags/freezes when performing segmentation as the program is taking computation resources. The process should take no longer than 1 minute on decent hardware (37s on i7-10875H CPU)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJgE8e5yoUqy"
      },
      "source": [
        "### Drive operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNhpiPkMDeWY",
        "outputId": "dcc93607-76a5-441c-d295-cf9335427a65"
      },
      "source": [
        "\"\"\"\n",
        "Mount Google Drive to access the datasets (datasets must be \n",
        "downloaded as there is no permanent link).\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4idtp4gvqmX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cf550f-768c-4147-b8a6-891102fd1bc5"
      },
      "source": [
        "# ! CHANGE this according to your storage structure\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/Brain\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/Brain\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3n5vU65mNLJ"
      },
      "source": [
        "\"\"\"\n",
        "Extract training data to the runtime's storage instead of Google Drive \n",
        "to avoid exceeding operation count and bandwidth quotas.\n",
        "\"\"\"\n",
        "\n",
        "import zipfile\n",
        "# ! CHANGE this according to your storage structure\n",
        "with zipfile.ZipFile('./MICCAI_BraTS2020_TrainingData.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MyPGDx4uJsz"
      },
      "source": [
        "## **Base code**\n",
        "Functions to read data, build and train model, and perform segmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sNjU3lHn3yz"
      },
      "source": [
        "### Custom checkpointing\n",
        "Source: https://github.com/keras-team/keras/issues/12803#issuecomment-614698317\n",
        "\n",
        "Custom callback, adding the new variable prev_best to ModelCheckpoint that corresponds to the loss of the saved model (inf loss if not specified) to preserve val_loss between Colab timeouts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjB0tStzn8p1"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class SaveModelCheckpoint(Callback):\n",
        "\n",
        "    def __init__(self, filepath, monitor='val_loss', verbose=0,\n",
        "                 save_best_only=False, save_weights_only=False,\n",
        "                 mode='auto', period=1, prev_best=None):\n",
        "        super(SaveModelCheckpoint, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.save_weights_only = save_weights_only\n",
        "        self.period = period\n",
        "        self.epochs_since_last_save = 0\n",
        "\n",
        "        if mode not in ['auto', 'min', 'max']:\n",
        "            warnings.warn('SaveModelCheckpoint mode %s is unknown, '\n",
        "                          'fallback to auto mode.' % (mode),\n",
        "                          RuntimeWarning)\n",
        "            mode = 'auto'\n",
        "        \n",
        "        if(prev_best == None):\n",
        "            if mode == 'min':\n",
        "                self.monitor_op = np.less\n",
        "                self.best = np.Inf\n",
        "            elif mode == 'max':\n",
        "                self.monitor_op = np.greater\n",
        "                self.best = -np.Inf\n",
        "            else:\n",
        "                if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
        "                    self.monitor_op = np.greater\n",
        "                    self.best = -np.Inf\n",
        "                else:\n",
        "                    self.monitor_op = np.less\n",
        "                    self.best = np.Inf\n",
        "        else:\n",
        "            if mode == 'min':\n",
        "                self.monitor_op = np.less\n",
        "                self.best = prev_best\n",
        "            elif mode == 'max':\n",
        "                self.monitor_op = np.greater\n",
        "                self.best = prev_best\n",
        "            else:\n",
        "                if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
        "                    self.monitor_op = np.greater\n",
        "                    self.best = prev_best\n",
        "                else:\n",
        "                    self.monitor_op = np.less\n",
        "                    self.best = prev_best\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epochs_since_last_save += 1\n",
        "        if self.epochs_since_last_save >= self.period:\n",
        "            self.epochs_since_last_save = 0\n",
        "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
        "            if self.save_best_only:\n",
        "                current = logs.get(self.monitor)\n",
        "                if current is None:\n",
        "                    warnings.warn('Can save best model only with %s available, '\n",
        "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
        "                else:\n",
        "                    if self.monitor_op(current, self.best):\n",
        "                        if self.verbose > 0:\n",
        "                            print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
        "                                  ' saving model to %s'\n",
        "                                  % (epoch + 1, self.monitor, self.best,\n",
        "                                     current, filepath))\n",
        "                        self.best = current\n",
        "                        if self.save_weights_only:\n",
        "                            self.model.save_weights(filepath, overwrite=True)\n",
        "                        else:\n",
        "                            self.model.save(filepath, overwrite=True)\n",
        "                    else:\n",
        "                        if self.verbose > 0:\n",
        "                            print('\\nEpoch %05d: %s did not improve from %0.5f' %\n",
        "                                  (epoch + 1, self.monitor, self.best))\n",
        "            else:\n",
        "                if self.verbose > 0:\n",
        "                    print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
        "                if self.save_weights_only:\n",
        "                    self.model.save_weights(filepath, overwrite=True)\n",
        "                else:\n",
        "                    self.model.save(filepath, overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NXlUI7bTB9G"
      },
      "source": [
        "### Read brain modalities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9VCIxxBS-O5"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from glob import glob\n",
        "\n",
        "def read_brain(brain_dir, mode='train', x0=42, x1=194, y0=29, y1=221, z0=2, z1=146):\n",
        "\n",
        "    \"\"\"\n",
        "    Reads and crops a brain's modalities (nii.gz format)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    brain_dir : string\n",
        "        The path to a folder that contains MRI modalities of a specific brain.\n",
        "    mode : string\n",
        "        'train' or 'validation' mode. \n",
        "    x0, x1, y0, y1, z0, z1 : int\n",
        "        The coordinates to crop brain volumes. \n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    all_modalities : array\n",
        "        The cropped modalities (+ gt if mode='train').\n",
        "    brain_affine : array\n",
        "        The affine matrix of the input brain volume.\n",
        "    brain_name : str\n",
        "        The name of the input brain volume.\n",
        "    \"\"\"\n",
        "\n",
        "    # path to the modalities of each brain\n",
        "    brain_dir = os.path.normpath(brain_dir)\n",
        "    flair     = glob(os.path.join(brain_dir, '*_flair*.nii.gz'))\n",
        "    t1        = glob(os.path.join(brain_dir, '*_t1*.nii.gz'))\n",
        "    t1ce      = glob(os.path.join(brain_dir, '*_t1ce*.nii.gz'))\n",
        "    t2        = glob(os.path.join(brain_dir, '*_t2*.nii.gz'))\n",
        "    \n",
        "    # 'train' mode has an additional ground truth modality\n",
        "    if mode=='train':\n",
        "        gt             = glob( os.path.join(brain_dir, '*_seg*.nii.gz'))\n",
        "        modalities_dir = [flair[0], t1[0], t1ce[0], t2[0], gt[0]]\n",
        "        \n",
        "    elif mode=='validation':\n",
        "        modalities_dir = [flair[0], t1[0], t1ce[0], t2[0]]   \n",
        "    \n",
        "    all_modalities = []    # numpy array containing all modalities\n",
        "    for modality in modalities_dir:      \n",
        "        nifti_file   = nib.load(modality) # load the NIfTI .nii modality image\n",
        "        brain_numpy  = np.asarray(nifti_file.dataobj) # create numpy array   \n",
        "        all_modalities.append(brain_numpy)\n",
        "        \n",
        "    # All modalities have the same affine, so take any one of them \n",
        "    # Affine is saved for preparing the predicted nii.gz file in the future.   \n",
        "    # See 'https://nipy.org/nibabel/coordinate_systems.html' for information on\n",
        "    # coordinate systems and affines in a nifti image    \n",
        "    brain_affine   = nifti_file.affine\n",
        "    all_modalities = np.array(all_modalities)\n",
        "    all_modalities = np.rint(all_modalities).astype(np.int16)\n",
        "    all_modalities = all_modalities[:, x0:x1, y0:y1, z0:z1]\n",
        "    # to fit keras channel last model\n",
        "    all_modalities = np.transpose(all_modalities) \n",
        "    brain_name     = os.path.basename(os.path.split(brain_dir)[0]) + '_' + os.path.basename(brain_dir) \n",
        "\n",
        "    return all_modalities, brain_affine, brain_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1_dqcH2igxT"
      },
      "source": [
        "### Prepare data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDrTspsoFsfT"
      },
      "source": [
        "import os\n",
        "import tables\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "    \n",
        "\n",
        "def create_table(dataset_dir, table_data_shape, save_dir, crop_coordinates, \n",
        "                 data_channels, k_fold=None):\n",
        "    \n",
        "    \"\"\"\n",
        "    Reads and saves all brain volumes into a single hdf5 table file. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset_dir : \n",
        "        The path to all brain volumes.\n",
        "    table_data_shape : tuple\n",
        "        A tuple which shows the final brain volume shape in the table.\n",
        "    data_channels : int\n",
        "        Number of data channels/modalities.\n",
        "    save_dir : str\n",
        "        The path to save table.\n",
        "    crop_coordinates : dict\n",
        "    k_fold : int\n",
        "        k-fold cross-validation\n",
        "        if specified, k .npy files will be saved. Each of these \n",
        "        shows the indices of the brain volumes in that fold, \n",
        "        which will be used for training the model.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    \n",
        "    all_brains_dir = glob(dataset_dir)  # path to all brain volumes\n",
        "    all_brains_dir.sort()\n",
        "    \n",
        "    hdf5_file    = tables.open_file(os.path.join(save_dir + 'data.hdf5'), mode='w')\n",
        "    filters      = tables.Filters(complevel=5, complib='blosc')\n",
        "    data_shape   = tuple([0] + list(table_data_shape) + [data_channels])\n",
        "    truth_shape  = tuple([0] + list(table_data_shape))\n",
        "    affine_shape = tuple([0] + [4, 4])\n",
        "    \n",
        "    data_storage   = hdf5_file.create_earray(hdf5_file.root, 'data', \n",
        "                                             tables.UInt16Atom(), shape=data_shape,\n",
        "                                             filters=filters, expectedrows=len(all_brains_dir))\n",
        "    \n",
        "    truth_storage  = hdf5_file.create_earray(hdf5_file.root, 'truth', \n",
        "                                             tables.UInt8Atom(), shape=truth_shape,\n",
        "                                             filters=filters, expectedrows=len(all_brains_dir))\n",
        "    \n",
        "    affine_storage = hdf5_file.create_earray(hdf5_file.root, 'affine', \n",
        "                                             tables.Float32Atom(), shape=affine_shape,\n",
        "                                             filters=filters, expectedrows=len(all_brains_dir))\n",
        "     \n",
        "    brain_names = []\n",
        "    for brain_dir in tqdm(all_brains_dir):\n",
        "        all_modalities, brain_affine, brain_name = read_brain(brain_dir, mode='train', **crop_coordinates)\n",
        "        brain    = all_modalities[..., :4]\n",
        "        gt       = all_modalities[..., -1]  # ground truth label and tumour mask\n",
        "        \n",
        "        # in BraTS there is no '3' label\n",
        "        gt[gt==4]  = 3    \n",
        "        brain_names.append(brain_name)   \n",
        "        data_storage.append(brain[np.newaxis,...])\n",
        "        truth_storage.append(gt[np.newaxis,...])\n",
        "        affine_storage.append(brain_affine[np.newaxis,...])\n",
        "        \n",
        "    hdf5_file.create_array(hdf5_file.root, 'brain_names', obj=brain_names)\n",
        "    hdf5_file.close()\n",
        "         \n",
        "    # for k-fold cross validation\n",
        "    if k_fold:\n",
        "        validation_split = (1/k_fold) \n",
        "        all_names = [i for i in brain_names]\n",
        "        \n",
        "        np.random.seed(100)\n",
        "        np.random.shuffle(all_names)\n",
        "              \n",
        "        val_size = int(validation_split * len(all_names))\n",
        "        \n",
        "        for fold in range(k_fold):\n",
        "            chosen_val = all_names[fold*val_size:(fold+1)*val_size]\n",
        "        \n",
        "            chosen_train = [i for i in all_names if i not in chosen_val]\n",
        "        \n",
        "            # saving train_idx is enough (val_idx is simply the indices not in train_idx)\n",
        "            train = chosen_train   \n",
        "            train_idx = [brain_names.index(i) for i in train]    \n",
        "            train_idx.sort()\n",
        "        \n",
        "            np.save(os.path.join(save_dir, 'fold{}_idx.npy'.format(fold)), train_idx)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_FxAkUAjJHr"
      },
      "source": [
        "### Custom data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hIUh-vmjL6c"
      },
      "source": [
        "\"\"\"\n",
        "Realtime multithread data generator\n",
        "\"\"\"\n",
        "\n",
        "import scipy\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from keras_preprocessing.image.affine_transformations import apply_affine_transform, transform_matrix_offset_center\n",
        "\n",
        "\n",
        "class CustomDataGenerator(Sequence):\n",
        "    \n",
        "    def __init__(self, hdf5_file, brain_idx, batch_size=16, view=\"axial\", \n",
        "                 mode='train', horizontal_flip=False, vertical_flip=False, \n",
        "                 rotation_range=0, zoom_range=0., shuffle=True):\n",
        "        \"\"\"\n",
        "        Custom data generator based on:\n",
        "        https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence  \n",
        "        \n",
        "        This implementation enables multiprocessing and on-the-fly augmentation \n",
        "        which speed up training.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        hdf5_file : file.File\n",
        "            hdf5 file that contains all data.\n",
        "        brain_idx : array\n",
        "            The brain indices corresponing to a specific fold. All of these \n",
        "            brain indices will be use for training and the ones which are \n",
        "            not in 'brain_idx' will be used for validation.\n",
        "        batch_size : int\n",
        "            The number of input/output arrays that will be generated each \n",
        "            time. \n",
        "        view : str\n",
        "            'axial', 'sagittal' or 'coronal'. The generator will extract\n",
        "            2D slices and perform normalisation with respect to the chosen view.\n",
        "        mode : str\n",
        "            Prepare the DataGenerator for 'train' or 'validation' phase. \n",
        "        horizontal_flip : bool\n",
        "            Whether to use horizontal flip for data augmentation.\n",
        "        vertical_flip : bool\n",
        "            Whether to use vertical flip for data augmentation.\n",
        "        rotation_range : float\n",
        "            Random rotation for data augmentation.\n",
        "        zoom_range : float\n",
        "            Random zoom for data augmentation.\n",
        "        shuffle : bool\n",
        "            Whether to shuffle data. If mode='validation' \n",
        "            shuffling will not be performed.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.data_storage    = hdf5_file.root.data    # brain data\n",
        "        self.truth_storage   = hdf5_file.root.truth   # tumour mask\n",
        "        \n",
        "        total_brains         = self.data_storage.shape[0]   # total number of brains\n",
        "        self.brain_idx       = self.get_brain_idx(brain_idx, mode, total_brains)\n",
        "        self.batch_size      = batch_size\n",
        "        \n",
        "        if view == 'axial':\n",
        "            self.view_axes = (0, 1, 2, 3)            \n",
        "        elif view == 'sagittal': \n",
        "            self.view_axes = (2, 1, 0, 3)\n",
        "        elif view == 'coronal':\n",
        "            self.view_axes = (1, 2, 0, 3)            \n",
        "        else:\n",
        "            ValueError('unknown input view => {}'.format(view))\n",
        "\n",
        "        self.mode            = mode\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.vertical_flip   = vertical_flip\n",
        "        self.rotation_range  = rotation_range       \n",
        "        self.zoom_range      = [1 - zoom_range, 1 + zoom_range]\n",
        "        self.shuffle         = shuffle\n",
        "        self.data_shape      = tuple(np.array(self.data_storage.shape[1:])[np.array(self.view_axes)])\n",
        "        \n",
        "        print('Using {} out of {} brains'.format(len(self.brain_idx), total_brains), end=' ')\n",
        "        print('({} out of {} 2D slices)'.format(len(self.brain_idx) * self.data_shape[0], total_brains * self.data_shape[0]))\n",
        "        print('the generated data shape in \"{}\" view: {}'.format(view, str(self.data_shape[1:])))\n",
        "        print('-----'*10)\n",
        "\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def get_brain_idx(brain_idx, mode, total_brains):\n",
        "        \n",
        "        \"\"\"\n",
        "        Getting the brain indices that will be used by the generator.\n",
        "        if mode=='train' => the original indices will be used (these npy files \n",
        "        were built based on training indices in 'Prepare data' for k-fold)\n",
        "        if mode=='validation' => the indices which are not in the brain_idx will\n",
        "        be used.\n",
        "        \"\"\"            \n",
        "\n",
        "        if mode=='validation':\n",
        "            brain_idx       = np.array([i for i in np.arange(total_brains) if i not in brain_idx])\n",
        "            print('DataGenerator is preparing for validation mode ...') \n",
        "        elif mode=='train':\n",
        "            brain_idx       = brain_idx\n",
        "            print('DataGenerator is preparing for training mode ...')\n",
        "        else:\n",
        "            raise ValueError('unknown \"{}\" mode'.format(mode))\n",
        "            \n",
        "        return brain_idx\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor( len(self.indices) / self.batch_size))\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Generate indices of the batch\n",
        "        idx = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        # Generate data\n",
        "        X_batch, Y_batch = self.data_load_and_preprocess(idx)\n",
        "\n",
        "        return X_batch, Y_batch\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Updates indices after each epoch\n",
        "        \"\"\"\n",
        "\n",
        "        tmp=[]\n",
        "        for i in self.brain_idx:\n",
        "            for j in range(self.data_shape[0]):\n",
        "                tmp.append((i,j))\n",
        "        self.indices = tmp\n",
        "            \n",
        "        if self.mode=='train' and self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "            \n",
        "\n",
        "    def read_data(self, brain_number, slice_number):\n",
        "        \n",
        "        \"\"\"\n",
        "        Reads data from table with respect to the 'view'\n",
        "        \"\"\"\n",
        "        \n",
        "        slice_    = self.data_storage[brain_number].transpose(self.view_axes)[slice_number]\n",
        "        label_    = self.truth_storage[brain_number].transpose(self.view_axes[:3])[slice_number]\n",
        "        label_    = np.expand_dims(label_, axis=-1)\n",
        "        \n",
        "        return slice_, label_ \n",
        "\n",
        "\n",
        "    def data_load_and_preprocess(self, idx):\n",
        "\n",
        "        \"\"\"\n",
        "        Generates data containing batch_size samples\n",
        "        \"\"\"\n",
        "\n",
        "        slice_batch = []\n",
        "        label_batch = []\n",
        "\n",
        "        # Generate data\n",
        "        for i in idx:\n",
        "            brain_number     = i[0]\n",
        "            slice_number     = i[1]\n",
        "            slice_, label_   = self.read_data(brain_number, slice_number)\n",
        "            slice_           = self.normalise_modalities(slice_)\n",
        "            slice_and_label  = np.concatenate((slice_, label_) , axis=-1)\n",
        "            params           = self.get_random_transform()\n",
        "            slice_and_label  = self.apply_transform(slice_and_label, params)\n",
        "            slice_           = slice_and_label[...,:4]\n",
        "            label_           = slice_and_label[..., 4]\n",
        "            label_           = to_categorical(label_, 4) \n",
        "            \n",
        "            slice_batch.append(slice_)\n",
        "            label_batch.append(label_)\n",
        "            \n",
        "        return np.array(slice_batch), np.array(label_batch)\n",
        "        \n",
        "    \n",
        "    def normalise_slice(self, slice):\n",
        "        \n",
        "        \"\"\"\n",
        "        Removes 1% of the top and bottom intensities and perform\n",
        "        normalisation on the input 2D slice.\n",
        "        \"\"\"\n",
        "\n",
        "        b = np.percentile(slice, 99)\n",
        "        t = np.percentile(slice, 1)\n",
        "        slice = np.clip(slice, t, b)\n",
        "        if np.std(slice)==0:\n",
        "            return slice\n",
        "        else:\n",
        "            slice = (slice - np.mean(slice)) / np.std(slice)\n",
        "            return slice\n",
        "        \n",
        "        \n",
        "    def normalise_modalities(self, Slice): \n",
        "        \n",
        "        \"\"\"\n",
        "        Performs normalisation on each modalities of input\n",
        "        \"\"\"\n",
        "\n",
        "        normalised_slices = np.zeros_like(Slice).astype(np.float32)\n",
        "        for slice_ix in range(4):\n",
        "            normalised_slices[..., slice_ix] = self.normalise_slice(Slice[..., slice_ix])\n",
        "    \n",
        "        return normalised_slices  \n",
        "    \n",
        "    \n",
        "    def get_random_transform(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Randomly generate parameters for affine transformation\n",
        "        \"\"\"\n",
        "    \n",
        "        if self.rotation_range:\n",
        "            theta = np.random.uniform(-self.rotation_range,self.rotation_range)    \n",
        "        else:\n",
        "            theta = 0            \n",
        " \n",
        "        if self.zoom_range[0] == 1 and self.zoom_range[1] == 1:\n",
        "            zx, zy = 1, 1\n",
        "        else:\n",
        "            zx, zy = np.random.uniform(self.zoom_range[0],self.zoom_range[1], 2)\n",
        "            \n",
        "        flip_horizontal = (np.random.random() < 0.5) * self.horizontal_flip    \n",
        "        flip_vertical   = (np.random.random() < 0.5) * self.vertical_flip\n",
        "        \n",
        "        transform_parameters = {'flip_horizontal': flip_horizontal,\n",
        "                                'flip_vertical':flip_vertical,\n",
        "                                'theta': theta, \n",
        "                                'zx': zx, \n",
        "                                'zy': zy}\n",
        "    \n",
        "        return transform_parameters    \n",
        "    \n",
        "\n",
        "    def flip_axis(self, x, axis):\n",
        "        \n",
        "        x = np.asarray(x).swapaxes(axis, 0)\n",
        "        x = x[::-1, ...]\n",
        "        x = x.swapaxes(0, axis)\n",
        "        return x\n",
        "\n",
        "    \n",
        "    def apply_transform(self, x, transform_parameters):\n",
        "        \n",
        "        \"\"\"\n",
        "        Apply affine transformation and axis flip\n",
        "        \"\"\"\n",
        "\n",
        "        x = apply_affine_transform(x, transform_parameters.get('theta', 0),\n",
        "                           transform_parameters.get('tx', 0),\n",
        "                           transform_parameters.get('ty', 0),\n",
        "                           transform_parameters.get('shear', 0),\n",
        "                           transform_parameters.get('zx', 1),\n",
        "                           transform_parameters.get('zy', 1),\n",
        "                           row_axis=0,\n",
        "                           col_axis=1,\n",
        "                           channel_axis=2,\n",
        "                           order=1)\n",
        "        \n",
        "        if transform_parameters.get('flip_horizontal', False):\n",
        "            x = self.flip_axis(x, 1)\n",
        "        if transform_parameters.get('flip_vertical', False):\n",
        "            x = self.flip_axis(x, 0)            \n",
        "        return x    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3cjrIOi8vj"
      },
      "source": [
        "### Loss\n",
        "Custom loss function using a combination of generalised dice loss and categorical cross-entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VofjIY1gi_cK"
      },
      "source": [
        "import tensorflow.keras.backend as bk\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "\n",
        "def generalised_dice(y_true, y_pred):\n",
        "    \n",
        "    \"\"\"\n",
        "    Generalised Dice Score\n",
        "    Source: https://arxiv.org/pdf/1707.03237\n",
        "    \"\"\"\n",
        "    \n",
        "    y_true    = bk.reshape(y_true,shape=(-1,4))\n",
        "    y_pred    = bk.reshape(y_pred,shape=(-1,4))\n",
        "    sum_p     = bk.sum(y_pred, -2)\n",
        "    sum_r     = bk.sum(y_true, -2)\n",
        "    sum_pr    = bk.sum(y_true * y_pred, -2)\n",
        "    weights   = bk.pow(bk.square(sum_r) + bk.epsilon(), -1)\n",
        "    generalised_dice = (2 * bk.sum(weights * sum_pr)) / (bk.sum(weights * (sum_r + sum_p)))\n",
        "    \n",
        "    return generalised_dice\n",
        "\n",
        "\n",
        "def generalised_dice_loss(y_true, y_pred):   \n",
        "    return 1-generalised_dice(y_true, y_pred)\n",
        "    \n",
        "    \n",
        "def custom_loss(y_true, y_pred):\n",
        "    \n",
        "    \"\"\"\n",
        "    Sum of generalised dice loss and categorical cross-entropy.\n",
        "    \"\"\"\n",
        "    \n",
        "    return generalised_dice_loss(y_true, y_pred) + categorical_crossentropy(y_true, y_pred)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_-HymUei2YI"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blpEqdOoi4o7"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Conv2D, PReLU, UpSampling2D, concatenate , Reshape, Dense, Permute, MaxPool2D, Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Activation, add, GaussianNoise, BatchNormalization, multiply\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "K.set_image_data_format(\"channels_last\")\n",
        "\n",
        "\n",
        "\n",
        "def unet_model(input_shape, learning_rate=0.01, start_channel=64, \n",
        "               number_of_levels=3, inc_rate=2, output_channels=4, saved_model_dir=None):\n",
        "    \"\"\"\n",
        "    Builds UNet model.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_shape : tuple\n",
        "        Shape of the input data (height, width, channel).\n",
        "    learning_rate : float\n",
        "        Learning rate for the model. \n",
        "    start_channel : int\n",
        "        Number of channels of the first conv layer. \n",
        "    number_of_levels : int\n",
        "        The depth of the U-structure.\n",
        "    inc_rate : int\n",
        "        Rate at which the conv channels increase. \n",
        "    output_channels : int\n",
        "        The number of output layer channels. \n",
        "    saved_model_dir : str\n",
        "        If specified, model weights will be loaded from this path. \n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    model : keras.model\n",
        "        The created keras model with respect to the input parameters.\n",
        "    \"\"\"\n",
        "\n",
        "        \n",
        "    input_layer = Input(shape=input_shape, name='input_layer')\n",
        "\n",
        "    x = GaussianNoise(0.05, name='gaussian_noise')(input_layer)\n",
        "    x = Conv2D(64, 2, padding='same')(x)\n",
        "    x = level_block(x, start_channel, number_of_levels, inc_rate)\n",
        "    x = BatchNormalization(axis = -1)(x)\n",
        "    x = PReLU(shared_axes=[1, 2])(x)\n",
        "\n",
        "    x            = Conv2D(output_channels, 1, padding='same')(x)\n",
        "    output_layer = Activation('softmax')(x)\n",
        "    \n",
        "    model        = Model(inputs = input_layer, outputs = output_layer)\n",
        "\n",
        "    if saved_model_dir:\n",
        "        model.load_weights(saved_model_dir)\n",
        "            \n",
        "    sgd = SGD(lr=learning_rate, momentum=0.9, decay=0)\n",
        "    model.compile(optimizer=sgd, loss=custom_loss)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def level_block(x, dim, level, inc):\n",
        "    \n",
        "    if level > 0:\n",
        "        m = res_block(x, dim, encoder_path=True)\n",
        "        x = Conv2D(int(inc*dim), 2, strides=2, padding='same')(m)\n",
        "        x = level_block(x, int(inc*dim), level-1, inc)\n",
        "\n",
        "        x = UpSampling2D(size=(2, 2))(x)\n",
        "        x = Conv2D(dim, 2, padding='same')(x)\n",
        "\n",
        "        m = concatenate([m,x])\n",
        "        x = res_block(m, dim, encoder_path=False)\n",
        "    else:\n",
        "        x = res_block(x, dim, encoder_path=True)\n",
        "    return x\n",
        "\n",
        "\n",
        "def res_block(x, dim, encoder_path=True):\n",
        "\n",
        "    m = BatchNormalization(axis = -1)(x)\n",
        "    m = PReLU(shared_axes = [1, 2])(m)\n",
        "    m = Conv2D(dim, 3, padding='same')(m)\n",
        "\n",
        "    m = BatchNormalization(axis = -1)(m)\n",
        "    m = PReLU(shared_axes = [1, 2])(m)\n",
        "    m = Conv2D(dim, 3, padding='same')(m)\n",
        "    m = Dropout(0.2)(m)\n",
        "\n",
        "    if encoder_path:\n",
        "        x = add([x, m])\n",
        "    else:\n",
        "        x = Conv2D(dim, 1, padding='same', use_bias=False)(x)\n",
        "        x = add([x,m])\n",
        "    return  x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAOUOK5ninjk"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXT36H17gptc"
      },
      "source": [
        "import os\n",
        "import tables\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "\n",
        "def train_model(hdf5_dir, brains_idx_dir, view, batch_size=16, val_batch_size=32,\n",
        "                lr=0.01, epochs=100, hor_flip=False, ver_flip=False, zoom_range=0.0, save_dir='./save/',\n",
        "                start_chs=64, levels=3, multiprocessing=False, load_model_dir=None, prev_best=None):\n",
        "    \"\"\"\n",
        "    Builds/loads the model, initialises data generators for \n",
        "    training and validation, then starts training.\n",
        "    \"\"\"\n",
        "    # preparing generators\n",
        "    hdf5_file        = tables.open_file(hdf5_dir, mode='r+')\n",
        "    brain_idx        = np.load(brains_idx_dir)\n",
        "    train_datagen    = CustomDataGenerator(hdf5_file, brain_idx, batch_size, view, 'train',\n",
        "                                    hor_flip, ver_flip, zoom_range, shuffle=True)\n",
        "    val_datagen      = CustomDataGenerator(hdf5_file, brain_idx, val_batch_size, view, 'validation', shuffle=False)\n",
        "    \n",
        "    # add callbacks    \n",
        "    save_dir     = os.path.join(save_dir, '{}_{}'.format(view, os.path.basename(brains_idx_dir)[:5]))\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "    logger       = CSVLogger(os.path.join(save_dir, 'log.txt'))\n",
        "    checkpointer = SaveModelCheckpoint(filepath=os.path.join(save_dir, 'model.hdf5'), verbose=1, \n",
        "                                                            save_best_only=True, prev_best=prev_best)\n",
        "    callbacks    = [logger, checkpointer]        \n",
        "    \n",
        "    # building the model\n",
        "    model_input_shape = train_datagen.data_shape[1:]\n",
        "    if load_model_dir:\n",
        "        model = unet_model(model_input_shape, lr, start_chs, levels, saved_model_dir=load_model_dir)\n",
        "        print(\"Model weights successfully loaded.\")\n",
        "    else:\n",
        "        model = unet_model(model_input_shape, lr, start_chs, levels)\n",
        "    print(\"The model has been built.\")\n",
        "    model.summary()\n",
        "    \n",
        "    # training the model\n",
        "    model.fit_generator(train_datagen, epochs=epochs, use_multiprocessing=multiprocessing, \n",
        "                        callbacks=callbacks, validation_data = val_datagen)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWc9KFwSir6u"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc4rJ9K2iZ6B"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from glob import glob\n",
        "from tensorflow.keras.models import load_model\n",
        "    \n",
        "\n",
        "def normalise_slice(slice):\n",
        "        \n",
        "        \"\"\"\n",
        "        Removes 1% of the top and bottom intensities and perform\n",
        "        normalisation on the input 2D slice.\n",
        "        \"\"\"\n",
        "\n",
        "        b = np.percentile(slice, 99)\n",
        "        t = np.percentile(slice, 1)\n",
        "        slice = np.clip(slice, t, b)\n",
        "        if np.std(slice)==0:\n",
        "            return slice\n",
        "        else:\n",
        "            slice = (slice - np.mean(slice)) / np.std(slice)\n",
        "            return slice\n",
        "\n",
        "\n",
        "def normalise_volume(input_volume):\n",
        "    \n",
        "    \"\"\"\n",
        "    Perform a slice-based normalisation on each modalities of input volume.\n",
        "    \"\"\"\n",
        "    normalised_slices = np.zeros_like(input_volume).astype(np.float32)\n",
        "    for slice_ix in range(4):\n",
        "        normalised_slices[slice_ix] = input_volume[slice_ix]\n",
        "        for mode_ix in range(input_volume.shape[1]):\n",
        "            normalised_slices[slice_ix][mode_ix] = normalise_slice(input_volume[slice_ix][mode_ix])\n",
        "\n",
        "    return normalised_slices    \n",
        "\n",
        "\n",
        "def save_predicted_results(prediction, brain_affine, view, output_dir,  \n",
        "                           z_main=155, z0=2, z1=146, y_main=240, y0=29, y1=221, \n",
        "                           x_main=240, x0=42, x1=194):\n",
        "    \n",
        "    \"\"\"\n",
        "    Save the segmented results into a .nii.gz file.\n",
        "    To correctly save the segmented brains, it is necessary to set x0, x1,... correctly.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    prediction : array\n",
        "        The predictred brain.\n",
        "    brain_affine : array\n",
        "        The affine matrix of the predicted brain volume.\n",
        "    view : str\n",
        "        'axial', 'sagittal' or 'coronal'. The 'view' is needed to reconstruct output axes.\n",
        "    output_dir : str\n",
        "        The path to save .nii.gz file.\n",
        "    \"\"\"\n",
        "    \n",
        "    prediction = np.argmax(prediction, axis=-1).astype(np.uint16)            \n",
        "    prediction[prediction==3] = 4\n",
        "    \n",
        "    if view==\"axial\":\n",
        "        prediction    = np.pad(prediction, ((z0, z_main-z1), (y0, y_main-y1), \n",
        "                                            (x0, x_main-x1)), 'constant')\n",
        "        prediction    = prediction.transpose(2,1,0)\n",
        "    elif view==\"sagital\":\n",
        "        prediction    = np.pad(prediction, ((x0, x_main-x1), (y0, y_main-y1), \n",
        "                                            (z0 , z_main-z1)), 'constant')\n",
        "    elif view==\"coronal\":\n",
        "        prediction    = np.pad(prediction, ((y0, y_main-y1), (x0, x_main-x1), \n",
        "                                            (z0 , z_main-z1)), 'constant')\n",
        "        prediction    = prediction.transpose(1,0,2)\n",
        "    \n",
        "    prediction_ni    = nib.Nifti1Image(prediction, brain_affine)\n",
        "    prediction_ni.to_filename(output_dir+ '.nii.gz')\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnqc8HXxlpZq"
      },
      "source": [
        "## **Execute workflow**\n",
        "Execute the below cells in order to obtain the trained model and predicted mask. \n",
        "\n",
        "**(!)** Compile all base code and the configuration before executing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrifO7UmjU8F"
      },
      "source": [
        "### Model training configuration\n",
        "For the training phase in the workflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVNz7tTwjWIy"
      },
      "source": [
        "cfg = dict()\n",
        "\n",
        "\"\"\"\n",
        "Model's current best val_loss (for custom checkpointing).\n",
        "\"\"\"\n",
        "cfg['prev_best']             = 0.69023\n",
        "\n",
        "\n",
        "# ! CHANGE these paths according to your storage structure\n",
        "\"\"\"\n",
        "If specified, before training, model weights will be loaded from this path.\n",
        "If None, train model from scratch.\n",
        "\"\"\"\n",
        "cfg['load_model_dir']        = './save/axial_fold0/model.hdf5' \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Path to all brain volumes (use '/*' since each brain's modalities are stored \n",
        "in a folder within the 'MICCAI_BraTS2020_TrainingData' folder)\n",
        "\"\"\"\n",
        "cfg['data_dir']              = '/content/MICCAI_BraTS2020_TrainingData/*'\n",
        "\n",
        "\n",
        "# the 'data' and 'save' folders will be created\n",
        "\"\"\"\n",
        "Path to save table file + k-fold files\n",
        "\"\"\"\n",
        "cfg['save_data_dir']         = './data/'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Path to save models + log files + tensorboards\n",
        "\"\"\"\n",
        "cfg['save_dir']              = './save/'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Default path of saved table.\n",
        "\"\"\"\n",
        "cfg['hdf5_dir']              = './data/data.hdf5'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Path to brain indices of a specific fold (if using k-fold)\n",
        "\"\"\"\n",
        "cfg['brains_idx_dir']        = './data/fold0_idx.npy'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "BraTS datasets contain 4 channels: (FLAIR, T1, T1ce, T2)\n",
        "\"\"\"\n",
        "cfg['data_channels']         = 4\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "k-fold cross-validation\n",
        "\"\"\"\n",
        "cfg['k_fold']                = 5\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Coordinates to crop brain volumes.  \n",
        "Final three shapes must be divisible by the network downscale rate.\n",
        "\"\"\"\n",
        "cfg['crop_coord']            = {'x0':42, 'x1':194,\n",
        "                                'y0':29, 'y1':221,\n",
        "                                'z0':2,  'z1':146}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Final data shapes of saved table file.\n",
        "\"\"\"\n",
        "cfg['table_data_shape']      = (cfg[\"crop_coord\"]['z1']-cfg[\"crop_coord\"]['z0'],\n",
        "                                cfg[\"crop_coord\"]['y1']-cfg[\"crop_coord\"]['y0'], \n",
        "                                cfg[\"crop_coord\"]['x1']-cfg[\"crop_coord\"]['x0'])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "'axial', 'sagittal' or 'coronal'. \n",
        "All 2D slices and the model will be prepared with respect to 'view'.\n",
        "\"\"\"\n",
        "cfg['view']                  = 'axial'\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Training and validation batch size\n",
        "\"\"\"\n",
        "cfg['batch_size']            = 32\n",
        "cfg['val_batch_size']        = 48\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Augmentation parameters.\n",
        "\"\"\"\n",
        "cfg['hor_flip']              = True\n",
        "cfg['ver_flip']              = True\n",
        "cfg['rotation_range']        = 0\n",
        "cfg['zoom_range']            = 0.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Learning rate and the number of epochs for training the model.\n",
        "\"\"\"\n",
        "cfg['epochs']                = 100\n",
        "cfg['lr']                    = 0.008\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "If True, use multithreading.\n",
        "\"\"\"\n",
        "cfg['multiprocessing']       = False \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Maximum number of processes when multithreading. \n",
        "If unspecified, workers default to 1. If 0, execute the generator \n",
        "on the main thread. \n",
        "\"\"\"\n",
        "cfg['workers']               = 10\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Depth of the U-Net architecture. \n",
        "\"\"\"\n",
        "cfg['levels']                = 3\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Number of channels of the first conv layer.\n",
        "\"\"\"\n",
        "cfg['start_chs']             = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEikKxwVtAgN"
      },
      "source": [
        "### Read and prepare data\n",
        "**(!)** Delete the 2 .csv files in the extracted folder so that the function does not read them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_6GgkzLlo42",
        "outputId": "32be922e-8848-4be6-f060-8e41fb2c5d41"
      },
      "source": [
        "\"\"\"\n",
        "Reads and saves all brain volumes into a single table file.\n",
        "\"\"\"\n",
        "\n",
        "create_table(cfg['data_dir'], cfg['table_data_shape'], cfg['save_data_dir'], \n",
        "             cfg['crop_coord'], cfg['data_channels'], cfg['k_fold'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 369/369 [05:59<00:00,  1.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJM9ATCJtW6I"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ywVLzmgrgjeQ",
        "outputId": "eef00080-825f-4b78-9b1b-80f88667db07"
      },
      "source": [
        "train_model(cfg['hdf5_dir'], cfg['brains_idx_dir'], cfg['view'], cfg['batch_size'], \n",
        "            cfg['val_batch_size'], cfg['lr'], cfg['epochs'], cfg['hor_flip'], \n",
        "            cfg['ver_flip'], cfg['zoom_range'], cfg['save_dir'], cfg['start_chs'], \n",
        "            cfg['levels'], cfg['multiprocessing'], cfg['load_model_dir'], \n",
        "            cfg['prev_best'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DataGenerator is preparing for training mode ...\n",
            "Using 296 out of 369 brains (42624 out of 53136 2D slices)\n",
            "the generated data shape in \"axial\" view: (192, 152, 4)\n",
            "--------------------------------------------------\n",
            "DataGenerator is preparing for validation mode ...\n",
            "Using 73 out of 369 brains (10512 out of 53136 2D slices)\n",
            "the generated data shape in \"axial\" view: (192, 152, 4)\n",
            "--------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model weights successfully loaded.\n",
            "The model has been built.\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_layer (InputLayer)        [(None, 192, 152, 4) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gaussian_noise (GaussianNoise)  (None, 192, 152, 4)  0           input_layer[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 192, 152, 64) 1088        gaussian_noise[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 192, 152, 64) 256         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_45 (PReLU)              (None, 192, 152, 64) 64          batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 192, 152, 64) 36928       p_re_lu_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 192, 152, 64) 256         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_46 (PReLU)              (None, 192, 152, 64) 64          batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 192, 152, 64) 36928       p_re_lu_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 192, 152, 64) 0           conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 192, 152, 64) 0           conv2d_75[0][0]                  \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 96, 76, 128)  32896       add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 96, 76, 128)  512         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_47 (PReLU)              (None, 96, 76, 128)  128         batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 96, 76, 128)  147584      p_re_lu_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 96, 76, 128)  512         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_48 (PReLU)              (None, 96, 76, 128)  128         batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 96, 76, 128)  147584      p_re_lu_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 96, 76, 128)  0           conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 96, 76, 128)  0           conv2d_78[0][0]                  \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 48, 38, 256)  131328      add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 48, 38, 256)  1024        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_49 (PReLU)              (None, 48, 38, 256)  256         batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 48, 38, 256)  590080      p_re_lu_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 48, 38, 256)  1024        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_50 (PReLU)              (None, 48, 38, 256)  256         batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 48, 38, 256)  590080      p_re_lu_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 48, 38, 256)  0           conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 48, 38, 256)  0           conv2d_81[0][0]                  \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 24, 19, 512)  524800      add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 24, 19, 512)  2048        conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_51 (PReLU)              (None, 24, 19, 512)  512         batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 24, 19, 512)  2359808     p_re_lu_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 24, 19, 512)  2048        conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_52 (PReLU)              (None, 24, 19, 512)  512         batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 24, 19, 512)  2359808     p_re_lu_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 24, 19, 512)  0           conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 24, 19, 512)  0           conv2d_84[0][0]                  \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_9 (UpSampling2D)  (None, 48, 38, 512)  0           add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 48, 38, 256)  524544      up_sampling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 48, 38, 512)  0           add_23[0][0]                     \n",
            "                                                                 conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 48, 38, 512)  2048        concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_53 (PReLU)              (None, 48, 38, 512)  512         batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 48, 38, 256)  1179904     p_re_lu_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 48, 38, 256)  1024        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_54 (PReLU)              (None, 48, 38, 256)  256         batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 48, 38, 256)  590080      p_re_lu_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 48, 38, 256)  131072      concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 48, 38, 256)  0           conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 48, 38, 256)  0           conv2d_90[0][0]                  \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_10 (UpSampling2D) (None, 96, 76, 256)  0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 96, 76, 128)  131200      up_sampling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 96, 76, 256)  0           add_22[0][0]                     \n",
            "                                                                 conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 96, 76, 256)  1024        concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_55 (PReLU)              (None, 96, 76, 256)  256         batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 96, 76, 128)  295040      p_re_lu_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 96, 76, 128)  512         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_56 (PReLU)              (None, 96, 76, 128)  128         batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 96, 76, 128)  147584      p_re_lu_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 96, 76, 128)  32768       concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 96, 76, 128)  0           conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 96, 76, 128)  0           conv2d_94[0][0]                  \n",
            "                                                                 dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_11 (UpSampling2D) (None, 192, 152, 128 0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 192, 152, 64) 32832       up_sampling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 192, 152, 128 0           add_21[0][0]                     \n",
            "                                                                 conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 192, 152, 128 512         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_57 (PReLU)              (None, 192, 152, 128 128         batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 192, 152, 64) 73792       p_re_lu_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 192, 152, 64) 256         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_58 (PReLU)              (None, 192, 152, 64) 64          batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 192, 152, 64) 36928       p_re_lu_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 192, 152, 64) 8192        concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 192, 152, 64) 0           conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 192, 152, 64) 0           conv2d_98[0][0]                  \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 192, 152, 64) 256         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_59 (PReLU)              (None, 192, 152, 64) 64          batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 192, 152, 4)  260         p_re_lu_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 192, 152, 4)  0           conv2d_99[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 10,159,748\n",
            "Trainable params: 10,153,092\n",
            "Non-trainable params: 6,656\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n",
            "1332/1332 [==============================] - 2148s 2s/step - loss: 0.1277 - val_loss: 0.7052\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.69023\n",
            "Epoch 2/100\n",
            "  13/1332 [..............................] - ETA: 29:32 - loss: 0.1185"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-089c797bb930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ver_flip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zoom_range'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_chs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'levels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multiprocessing'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'load_model_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m             cfg['prev_best'])\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-a13af6e428d4>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(hdf5_dir, brains_idx_dir, view, batch_size, val_batch_size, lr, epochs, hor_flip, ver_flip, zoom_range, save_dir, start_chs, levels, multiprocessing, load_model_dir, prev_best)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     model.fit_generator(train_datagen, epochs=epochs, use_multiprocessing=multiprocessing, \n\u001b[0;32m---> 41\u001b[0;31m                         callbacks=callbacks, validation_data = val_datagen)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1955\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1957\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6UNvrdJs2IA"
      },
      "source": [
        "### Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trnbj76qI4j0"
      },
      "source": [
        "\"\"\"\n",
        "Extract testing data to the runtime's storage instead of Google Drive \n",
        "to avoid exceeding operation count and bandwidth quotas.\n",
        "\"\"\"\n",
        "\n",
        "import zipfile\n",
        "# ! CHANGE this according to your storage structure\n",
        "with zipfile.ZipFile('/content/gdrive/MyDrive/Colab Notebooks/42028/Assignment 3/MICCAI_BraTS2020_ValidationData.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMcIlkDO0OXk",
        "outputId": "79098507-510f-4508-bf82-f3874abd82b7"
      },
      "source": [
        "\"\"\"\n",
        "Obtain the predicted tumour masks for the test dataset.\n",
        "\"\"\"\n",
        "# ! CHANGE these paths according to your storage structure\n",
        "val_data_dir       = '/content/MICCAI_BraTS2020_ValidationData/*'\n",
        "saved_model_dir    = '/content/gdrive/MyDrive/Colab Notebooks/42028/Assignment 3/save/axial_fold0/model.hdf5'\n",
        "\n",
        "# 'predict' folder will be created\n",
        "save_pred_dir      = './predict/'\n",
        "view               = 'axial'\n",
        "batch_size         = 32\n",
        "\n",
        "\n",
        "if not os.path.isdir(save_pred_dir):\n",
        "    os.mkdir(save_pred_dir)\n",
        "    \n",
        "all_brains_dir = glob(val_data_dir)\n",
        "all_brains_dir.sort()\n",
        "\n",
        "if view == 'axial':\n",
        "    view_axes = (0, 1, 2, 3)            \n",
        "elif view == 'sagittal': \n",
        "    view_axes = (2, 1, 0, 3)\n",
        "elif view == 'coronal':\n",
        "    view_axes = (1, 2, 0, 3)            \n",
        "else:\n",
        "    ValueError('unknown input view => {}'.format(view))\n",
        "\n",
        "model        = load_model(saved_model_dir, compile=False)\n",
        "for brain_dir in all_brains_dir:    \n",
        "    if os.path.isdir(brain_dir):\n",
        "        print(\"Volume ID: \", os.path.basename(brain_dir))\n",
        "        all_modalities, brain_affine, _ = read_brain(brain_dir, mode='validation')\n",
        "        all_modalities                  = all_modalities.transpose(view_axes)\n",
        "        all_modalities                  = normalise_volume(all_modalities)\n",
        "        prediction                      = model.predict(all_modalities, batch_size=batch_size, verbose=1)\n",
        "        output_dir                      = os.path.join(save_pred_dir, os.path.basename(brain_dir))\n",
        "        save_predicted_results(prediction, brain_affine, view, output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Volume ID:  BraTS20_Validation_001\n",
            "5/5 [==============================] - 2s 393ms/step\n",
            "Volume ID:  BraTS20_Validation_002\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_003\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_004\n",
            "5/5 [==============================] - 1s 225ms/step\n",
            "Volume ID:  BraTS20_Validation_005\n",
            "5/5 [==============================] - 1s 228ms/step\n",
            "Volume ID:  BraTS20_Validation_006\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_007\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_008\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_009\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_010\n",
            "5/5 [==============================] - 1s 228ms/step\n",
            "Volume ID:  BraTS20_Validation_011\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_012\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_013\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_014\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_015\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_016\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_017\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_018\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_019\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_020\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_021\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_022\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_023\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_024\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_025\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_026\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_027\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_028\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_029\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_030\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_031\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_032\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_033\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_034\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_035\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_036\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_037\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_038\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_039\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_040\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_041\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_042\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_043\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_044\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_045\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_046\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_047\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_048\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_049\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_050\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_051\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_052\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_053\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_054\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_055\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_056\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_057\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_058\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_059\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_060\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_061\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_062\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_063\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_064\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_065\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_066\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_067\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_068\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_069\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_070\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_071\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_072\n",
            "5/5 [==============================] - 1s 228ms/step\n",
            "Volume ID:  BraTS20_Validation_073\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_074\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_075\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_076\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_077\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_078\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_079\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_080\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_081\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_082\n",
            "5/5 [==============================] - 1s 228ms/step\n",
            "Volume ID:  BraTS20_Validation_083\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_084\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_085\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_086\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_087\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_088\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_089\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_090\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_091\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_092\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_093\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_094\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_095\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_096\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_097\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_098\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_099\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_100\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_101\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_102\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_103\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_104\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_105\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_106\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_107\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_108\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_109\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_110\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_111\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_112\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_113\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_114\n",
            "5/5 [==============================] - 1s 226ms/step\n",
            "Volume ID:  BraTS20_Validation_115\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_116\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_117\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_118\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_119\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_120\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_121\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_122\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_123\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_124\n",
            "5/5 [==============================] - 1s 227ms/step\n",
            "Volume ID:  BraTS20_Validation_125\n",
            "5/5 [==============================] - 1s 227ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJS1vtZNrypr"
      },
      "source": [
        "### GUI code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWunO9lnr9D3"
      },
      "source": [
        "# Import Required Libraries\n",
        "\n",
        "from tkinter import *\n",
        "from tkinter import messagebox\n",
        "from tkinter import filedialog\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "import os\n",
        "import nibabel as nib\n",
        "from glob import glob\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "# import cv2 <-- To use openCV function/methods\n",
        "\n",
        "# Create a Window.\n",
        "MyWindow = Tk() # Create a window\n",
        "MyWindow.title(\"Brain Tumour Segmentation\") # Change the Title of the GUI\n",
        "MyWindow.geometry('1024x768') # Set the size of the Windows\n",
        "\n",
        "\n",
        "header_label = Label(MyWindow)\n",
        "header_label.place(relx=.5, rely=0.1, anchor='center')\n",
        "word = 'Brain Tumour Segmentation'\n",
        "header_label.configure(text = word, font= (\"Arial\", 42))\n",
        "\n",
        "\n",
        "\n",
        "def read_brain(brain_dir, mode='train', x0=42, x1=194, y0=29, y1=221, z0=2, z1=146):\n",
        "\n",
        "    \"\"\"\n",
        "    Reads and crops a brain's modalities (nii.gz format)\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    brain_dir : string\n",
        "        The path to a folder that contains MRI modalities of a specific brain\n",
        "    mode : string\n",
        "        'train' or 'validation' mode. \n",
        "    x0, x1, y0, y1, z0, z1 : int\n",
        "        The coordinates to crop brain volumes. \n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    all_modalities : array\n",
        "        The cropped modalities (+ gt if mode='train')\n",
        "    brain_affine : array\n",
        "        The affine matrix of the input brain volume\n",
        "    brain_name : str\n",
        "        The name of the input brain volume\n",
        "    \"\"\"\n",
        "\n",
        "    # path to the modalities of each brain\n",
        "    brain_dir = os.path.normpath(brain_dir)\n",
        "    flair     = glob(os.path.join(brain_dir, '*_flair*.nii.gz'))\n",
        "    t1        = glob(os.path.join(brain_dir, '*_t1*.nii.gz'))\n",
        "    t1ce      = glob(os.path.join(brain_dir, '*_t1ce*.nii.gz'))\n",
        "    t2        = glob(os.path.join(brain_dir, '*_t2*.nii.gz'))\n",
        "    \n",
        "    # 'train' mode has an additional ground truth modality\n",
        "    if mode=='train':\n",
        "        gt             = glob( os.path.join(brain_dir, '*_seg*.nii.gz'))\n",
        "        modalities_dir = [flair[0], t1[0], t1ce[0], t2[0], gt[0]]\n",
        "        \n",
        "    elif mode=='validation':\n",
        "        modalities_dir = [flair[0], t1[0], t1ce[0], t2[0]]   \n",
        "    \n",
        "    all_modalities = []    # numpy array containing all modalities\n",
        "    for modality in modalities_dir:      \n",
        "        nifti_file   = nib.load(modality) # load the NIfTI .nii modality image\n",
        "        brain_numpy  = np.asarray(nifti_file.dataobj) # create numpy array   \n",
        "        all_modalities.append(brain_numpy)\n",
        "        \n",
        "    # all modalities have the same affine, so we take one of them (the last one in this case),\n",
        "    # affine is just saved for preparing the predicted nii.gz file in the future.       \n",
        "    brain_affine   = nifti_file.affine\n",
        "    all_modalities = np.array(all_modalities)\n",
        "    all_modalities = np.rint(all_modalities).astype(np.int16)\n",
        "    all_modalities = all_modalities[:, x0:x1, y0:y1, z0:z1]\n",
        "    # to fit keras channel last model\n",
        "    all_modalities = np.transpose(all_modalities) \n",
        "    # tumor grade + name\n",
        "    brain_name     = os.path.basename(os.path.split(brain_dir)[0]) + '_' + os.path.basename(brain_dir) \n",
        "\n",
        "    return all_modalities, brain_affine, brain_name\n",
        "\n",
        "\n",
        "def normalize_slice(slice):\n",
        "        \n",
        "        \"\"\"\n",
        "        Removes 1% of the top and bottom intensities and perform\n",
        "        normalization on the input 2D slice.\n",
        "        \"\"\"\n",
        "        b = np.percentile(slice, 99)\n",
        "        t = np.percentile(slice, 1)\n",
        "        slice = np.clip(slice, t, b)\n",
        "        if np.std(slice)==0:\n",
        "            return slice\n",
        "        else:\n",
        "            slice = (slice - np.mean(slice)) / np.std(slice)\n",
        "            return slice\n",
        "\n",
        "\n",
        "def normalize_volume(input_volume):\n",
        "    \n",
        "    \"\"\"\n",
        "    Perform a slice-based normalization on each modalities of input volume.\n",
        "    \"\"\"\n",
        "    normalized_slices = np.zeros_like(input_volume).astype(np.float32)\n",
        "    for slice_ix in range(4):\n",
        "        normalized_slices[slice_ix] = input_volume[slice_ix]\n",
        "        for mode_ix in range(input_volume.shape[1]):\n",
        "            normalized_slices[slice_ix][mode_ix] = normalize_slice(input_volume[slice_ix][mode_ix])\n",
        "\n",
        "    return normalized_slices    \n",
        "\n",
        "\n",
        "def save_predicted_results(prediction, brain_affine, view, output_dir,  \n",
        "                           z_main=155, z0=2, z1=146, y_main=240, y0=29, y1=221, \n",
        "                           x_main=240, x0=42, x1=194):\n",
        "    \n",
        "    \"\"\"\n",
        "    Save the segmented results into a .nii.gz file.\n",
        "    To correctly save the segmented brains, it is necessery to set x0, x1, ... correctly.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    prediction : array\n",
        "        The predictred brain.\n",
        "    brain_affine : array\n",
        "        The affine matrix of the predicted brain volume\n",
        "    view : str\n",
        "        'axial', 'sagittal' or 'coronal'. The 'view' is needed to reconstruct output axes.\n",
        "    output_dir : str\n",
        "        The path to save .nii.gz file.\n",
        "    \"\"\"\n",
        "    \n",
        "    prediction = np.argmax(prediction, axis=-1).astype(np.uint16)            \n",
        "    prediction[prediction==3] = 4\n",
        "    \n",
        "    if view == \"axial\":\n",
        "        prediction    = np.pad(prediction, ((z0, z_main-z1), (y0, y_main-y1), (x0, x_main-x1)), 'constant')\n",
        "        prediction    = prediction.transpose(2,1,0)\n",
        "    elif view == \"sagital\":\n",
        "        prediction    = np.pad(prediction, ((x0, x_main-x1), (y0, y_main-y1), (z0 , z_main-z1)), 'constant')\n",
        "    elif view == \"coronal\":\n",
        "        prediction    = np.pad(prediction, ((y0, y_main-y1), (x0, x_main-x1), (z0 , z_main-z1)), 'constant')\n",
        "        prediction    = prediction.transpose(1,0,2)\n",
        "    #\n",
        "    prediction_ni    = nib.Nifti1Image(prediction, brain_affine)\n",
        "    prediction_ni.to_filename(output_dir + 'prediction.nii.gz')\n",
        "       \n",
        "\n",
        "view               = 'axial'\n",
        "\n",
        "if view == 'axial':\n",
        "    view_axes = (0, 1, 2, 3)            \n",
        "elif view == 'sagittal': \n",
        "    view_axes = (2, 1, 0, 3)\n",
        "elif view == 'coronal':\n",
        "    view_axes = (1, 2, 0, 3)            \n",
        "else:\n",
        "    ValueError('unknown input view => {}'.format(view))\n",
        "\n",
        "        \n",
        "def predict_single(saved_model, folder):\n",
        "    model        = load_model(saved_model, compile=False)    \n",
        "    if os.path.isdir(folder):\n",
        "        print(\"Volume ID: \", os.path.basename(folder))\n",
        "        all_modalities, brain_affine, _ = read_brain(folder, mode='validation')\n",
        "        all_modalities                  = all_modalities.transpose(view_axes)\n",
        "        all_modalities                  = normalize_volume(all_modalities)\n",
        "        prediction                      = model.predict(all_modalities, verbose=1)\n",
        "        save_predicted_results(prediction, brain_affine, view, folder)\n",
        "        open_image(folder + 'prediction.nii.gz')\n",
        "\n",
        "\n",
        "# Methods for processing the brain modalities using trained model\n",
        "\n",
        "def read_img(img_path):\n",
        "    return sitk.GetArrayFromImage(sitk.ReadImage(img_path))\n",
        "\n",
        "\n",
        "def open_folder(folder):\n",
        "    messagebox.showinfo(\"Chosen Folder\", folder)\n",
        "    t1 = glob(os.path.join(folder,'*t1.nii.gz'))\n",
        "    t2 = glob(os.path.join(folder,'*t2.nii.gz'))\n",
        "    flair = glob(os.path.join(folder,'*flair.nii.gz'))\n",
        "    t1ce = glob(os.path.join(folder,'*t1ce.nii.gz'))\n",
        "    for i in t1:\n",
        "        img = (read_img(i)[100]).astype(np.uint8)\n",
        "        cv2.imshow('brain', img)\n",
        "\n",
        "# Open Image Function using OpenCV\n",
        "def open_image(image):\n",
        "    messagebox.showinfo(\"Image to Show\", image)\n",
        "    # Open the image using OpenCV\n",
        "    img = nib.load(image)\n",
        "    data = img.get_fdata()\n",
        "    cv2.imshow('segmentation', data[:, :, data.shape[2] // 2].T)\n",
        "\n",
        "\n",
        "# Event Methods attached to the buttons\n",
        "def BttnOpen_Clicked():\n",
        "    messagebox.showinfo(\"Info\", \"Select the pretrained model.\")\n",
        "    saved_model = filedialog.askopenfilename(filetypes = ((\"hdf5 files\",\"*.hdf5\"),(\"h5 files\",\"*.h5\"),(\"pb files\",\"*.pb\")))\n",
        "    messagebox.showinfo(\"Info\", \"Pretrained model selected.\")\n",
        "    \n",
        "    messagebox.showinfo(\"Info\", \"Choose folder containing the brain modalities (4 nii.gz files).\")\n",
        "    folder = filedialog.askdirectory()\n",
        "    open_folder(folder) \n",
        "    messagebox.showinfo(\"Info\", \"A modality slice has been shown in the newly opened window (brain).\")\n",
        "    \n",
        "    messagebox.showinfo(\"Info\", \"Click OK to start segmentation. A popup will show when the segmentation process has been completed.\")\n",
        "    predict_single(saved_model, folder)\n",
        "    \n",
        "    messagebox.showinfo(\"Info\", \"Segmentation finished. The predicted tumour mask is displayed (segmentation) and the \" +\n",
        "                        \"resulting nii.gz file has been saved in the same parent folder as the folder containing the brain modalities.\")\n",
        "\n",
        "\n",
        "def BttnProcess_Clicked():\n",
        "    file = filedialog.askopenfilename(filetypes = ((\"NifTI files\",\"*.nii.gz\"),(\"all files\",\"*.*\")))\n",
        "    open_image(file)\n",
        "    \n",
        "    # messagebox.showinfo(\"Info\", \"Process Button Clicked\")\n",
        "    # predict_single(folder)\n",
        "    \n",
        "    \n",
        "    # Testing\n",
        "    # messagebox.showwarning(\"Invalid Input\",\"Image is having an invalid format\") # Showing Warning not very Critcal \n",
        "    # messagebox.showerror(\"Invalid Input\",\"Image is having an invalid format\") # Showing Error, very Critcal \n",
        "    # classifcationResult = \"CAT\"\n",
        "    # messagebox.showinfo(\"Classfication Result\", classifcationResult)\n",
        "    # result = \"DOG\"\n",
        "    # resultText = \"Classification Result:\" + result  # Concatenate the result class to the Label on the Window\n",
        "    # ClassficationResultLabel.configure(text = resultText)  # Update the Label text on the Window\n",
        "    \n",
        "    \n",
        "\n",
        "# Add the Components created previsously to the window\n",
        "\n",
        "InputLabel = Label(text=\"Input Image\")\n",
        "InputLabel.place(x=300, y=200)\n",
        "SegLabel = Label(text=\"Segmented Image\")\n",
        "SegLabel.place(x=600, y=200)\n",
        "BrowseBttn = Button(text=\"Browse\", command=BttnOpen_Clicked)\n",
        "BrowseBttn.place(x=0, y=180)\n",
        "ProcessBttn = Button(text=\"Tumour Area %\", command=BttnProcess_Clicked)\n",
        "ProcessBttn.place(x=300, y=500)\n",
        "ProcessBttn = Button(text=\"Tumour Grade\", command=BttnProcess_Clicked)\n",
        "ProcessBttn.place(x=600, y=500)\n",
        "\n",
        "# Calling the mainloop()\n",
        "MyWindow.mainloop()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}